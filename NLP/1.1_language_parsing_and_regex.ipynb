{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project illustrates how NLP preprocessing and parsing techniques can be used to uncover the prvalant grammatical structure of a text, which later may possibly used for sentiment analysis by feeding most common grammatical structures data to a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\"Please note that I am not the author of these functions. I have found \n",
    "   these functionsin a NLP topic in codeacademy website.\"\"\"\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "\n",
    "def word_sentence_tokenize(text):\n",
    "  \n",
    "  # create a PunktSentenceTokenizer\n",
    "  sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "  \n",
    "  # sentence tokenize text\n",
    "  sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "  \n",
    "  # create a list to hold word tokenized sentences\n",
    "  word_tokenized = list()\n",
    "  \n",
    "  # for-loop through each tokenized sentence in sentence_tokenized\n",
    "  for tokenized_sentence in sentence_tokenized:\n",
    "    # word tokenize each sentence and append to word_tokenized\n",
    "    word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "    \n",
    "  return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1899', 'contents', 'introduction', '.']\n",
      "[('1899', 'CD'), ('contents', 'NNS'), ('introduction', 'NN'), ('.', '.')]\n",
      "Most common NP chunks: \n",
      "\n",
      " [((('hector', 'NN'),), 321), ((('i', 'NN'),), 276), ((('jove', 'NN'),), 257), ((('troy', 'NN'),), 208), ((('vain', 'NN'),), 195), ((('war', 'NN'),), 192), ((('son', 'NN'),), 167), ((('the', 'DT'), ('plain', 'NN')), 157), ((('the', 'DT'), ('field', 'NN')), 154), ((('thou', 'NN'),), 153), ((('the', 'DT'), ('ground', 'NN')), 138), ((('death', 'NN'),), 134), ((('hand', 'NN'),), 132), ((('greece', 'NN'),), 126), ((('heaven', 'NN'),), 125), ((('fate', 'NN'),), 123), ((('thee', 'NN'),), 122), ((('breast', 'NN'),), 121), ((('the', 'DT'), ('trojan', 'NN')), 120), ((('the', 'DT'), ('god', 'NN')), 119), ((('the', 'DT'), ('war', 'NN')), 117), ((('the', 'DT'), ('greeks', 'NN')), 116), ((('blood', 'NN'),), 113), ((('homer', 'NN'),), 112), ((('the', 'DT'), ('king', 'NN')), 105), ((('force', 'NN'),), 102), ((('rage', 'NN'),), 100), ((('care', 'NN'),), 98), ((('man', 'NN'),), 97), ((('head', 'NN'),), 97)]\n",
      "Most common VP chunks: \n",
      "\n",
      " [(((\"'t\", 'NN'), ('is', 'VBZ')), 19), ((('i', 'NN'), ('am', 'VBP')), 11), (((\"'t\", 'NN'), ('was', 'VBD')), 11), ((('the', 'DT'), ('hero', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('saw', 'VBD')), 8), ((('the', 'DT'), ('scene', 'NN'), ('lies', 'VBZ')), 7), ((('i', 'NN'), ('was', 'VBD')), 6), ((('confess', 'NN'), (\"'d\", 'VBD')), 6), ((('the', 'DT'), ('scene', 'NN'), ('is', 'VBZ')), 6), ((('view', 'NN'), (\"'d\", 'VBD')), 5), ((('i', 'NN'), ('felt', 'VBD')), 5), ((('i', 'NN'), ('bear', 'VBP')), 5), ((('hector', 'NN'), ('is', 'VBZ')), 5), ((('vain', 'NN'), ('was', 'VBD')), 5), ((('homer', 'NN'), ('was', 'VBD')), 4), ((('i', 'NN'), ('have', 'VBP')), 4), ((('hunger', 'NN'), ('was', 'VBD')), 4), ((('glory', 'NN'), ('lost', 'VBN')), 4), ((('i', 'NN'), ('see', 'VBP')), 4), ((('war', 'NN'), ('be', 'VB')), 4), ((('the', 'DT'), ('weapon', 'NN'), ('stood', 'VBD')), 4), ((('i', 'NN'), ('go', 'VBP')), 4), ((('the', 'DT'), ('silence', 'NN'), ('broke', 'VBD')), 4), ((('the', 'DT'), ('trojan', 'NN'), ('bands', 'VBZ')), 4), ((('father', 'NN'), ('gave', 'VBD')), 4), ((('i', 'NN'), ('deem', 'VBP')), 4), ((('minerva', 'NN'), ('repressing', 'VBG')), 3), ((('thetis', 'NN'), ('calling', 'VBG')), 3), ((('thetis', 'NN'), ('entreating', 'VBG')), 3)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "\n",
    "# import text of choice here\n",
    "with open('The illiad.txt', encoding='utf-8') as fileObject:\n",
    "  text = fileObject.read().lower()\n",
    "\n",
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence here\n",
    "\n",
    "single_word_tokenized_sentence = word_tokenized_text[1]\n",
    "print(single_word_tokenized_sentence)\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for sentence in word_tokenized_text:\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "  pos_tagged_text.append(pos_tag(sentence))\n",
    "\n",
    "# store and print any part-of-speech tagged sentence here\n",
    "single_pos_sentence = pos_tagged_text[1]\n",
    "print(single_pos_sentence)\n",
    "\n",
    "# define noun phrase chunk grammar here\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ.?>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object here\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ.?>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create verb phrase RegexpParser object here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "\n",
    "# create a for loop through each pos-tagged sentence here\n",
    "for pos_tagged_sentence in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "  np_chunked_text.append(np_chunk_parser.parse(pos_tagged_sentence))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# Lets see how 'np_chunked_text' looks like\n",
    "# print(\"This is np_chunked_text's raw content: \\n\\n\")\n",
    "# print(np_chunked_text)\n",
    "\n",
    "# store and print the most common NP-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print('Most common NP chunks: \\n\\n', most_common_np_chunks)\n",
    "\n",
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print('Most common VP chunks: \\n\\n', most_common_vp_chunks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
